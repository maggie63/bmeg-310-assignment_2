---
title: "Assignment 2"
author: "Maggie Wang (61851572), Sogand Golshahian (78138914), Elias Krapf (78968195)"
date: "2023-10-22"
output:
  pdf_document: default
  html_document: default
---

## Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r, load libraries, message = FALSE}
# Load required libraries
library(ggplot2)
library(caret)
library(ROCR)
library(randomForest)
```

```{r}
# Read the data
ovarian.data <- na.omit(read.delim("ovarian.data", sep=",", header = FALSE))
features <- c("perimeter", "area", "smoothness", "symmetry", "concavity", 
              paste("protein", seq(1, 25), sep=""))
names(ovarian.data) <- c("cell_id", "diagnosis", features) 
# paste0(features,"_mean"), paste0(features,"_se"), paste0(features,"_worst"))

# Look at the dimensions and the first few rows of the data
dim(ovarian.data)
head(ovarian.data)
```

## Q1. Dimensionality Reduction

\n
**Q1.1. Perform PCA on the features of the data set. How much of the variation in the data is associated with PC1?**
\n

```{r, Q1.1}
# Exclude the cell_id column and the diagnosis column
ovarian.data.subset <- ovarian.data[, 3:32]

# Perform PCA
ovarian.pca <- prcomp(ovarian.data.subset, center = TRUE,scale. = TRUE)
summary(ovarian.pca)
```

About 42.77% of the variation in the data is associated with PC1.

\n
**Q1.2. You want to represent 90% of the variance in the data by dimensionality reduction. How many PCs do you need to achieve this? In other words, what would be the dimensionality of the reduced feature space so that you preserve 90% of the variability in the data?**
\n

To represent 90% of the variance in the data by dimensionality reduction, you would need about 9 PCs. This is because the cumulative proportion of variance at PC9 is 90.1%, whereas at PC8 it was 88.6%.

\n
**Q1.3. As you should know by now, PCA transforms the data into a new space. In a 2-D plot, can you plot the observations corresponding to the first two important PCs? Note, use two different colors to represent the two classes of cells.**
\n

```{r, Q1.3}
# Create a data frame with the first two PCs and diagnosis
pca1_pc2_data <- data.frame(PC1 = ovarian.pca$x[, 1], PC2 = ovarian.pca$x[, 2], 
                       Diagnosis = ovarian.data$diagnosis)

# Create the plot
ggplot(pca1_pc2_data, aes(PC1, PC2)) +
  geom_point(aes(color = Diagnosis))
```

\n
**Q1.4. Can you plot the "area" and "concavity" features associated with the cells?**
\n

```{r, Q1.4}
# Create a data frame with "area" and "concavity" features
area_concavity_data <- data.frame(Area = ovarian.data$area, 
                                  Concavity = ovarian.data$concavity, 
                                  Diagnosis = ovarian.data$diagnosis)

# Create the plot
ggplot(area_concavity_data, aes(Area, Concavity)) +
  geom_point(aes(color = Diagnosis)) +
  labs(title = "Concavity vs. Area Plot") +
  theme(plot.title = element_text(hjust = 0.5))
```

\n
**Q1.5. What is the difference between the two plots? Which one gives you better separation between the classes and why?**
\n

This first difference that we notice between the plots is that the axes of the "area" vs "concavity" plot have not been scaled or centered. Additionally, the plot of "area" vs "concavity" does not show good separation between the benign and malignant cells, as there seems to be more of a homogeneous mix between the two types of cells in the plot. Alternatively, the plot of "PC1" vs "PC2" shows great separation between the classes of cells, as the benign and malignant cells can almost completely be separated using a line on the plot. This is because the first two PCs have the highest proportion of the variation in the data set, so they will have the most difference between them.

\n
**Q1.6. (bonus): Plot the distribution of the PCs. Hint: you can use box plot on the transformed data set.**
\n

```{r, Q1.6 (bonus)}
# Create a data frame of the principal components
pca_data_df <- as.data.frame(ovarian.pca$x)

# Create a box plot
boxplot(pca_data_df, main = "Distribution of Principal Components", 
        xlab = "Principal Component", ylab = "Value")
```

## Q2. Clustering

\n
**Q2.1. Apply k-means clustering on the data and identify two clusters within your data set. What is the concordance between the clusters that you have identified and the true labels of the cells (Benign vs Malignant).**
\n

```{r, Q2.1}
# Scale the data
ovarian.scaled <- scale(ovarian.data.subset)

# Set seed to get reproducible results (only used for this question)
#set.seed(123)

# Perform k-means clustering with 2 clusters
km.out <- kmeans(ovarian.scaled, centers = 2, iter.max = 1, nstart = 20)
cluster.labels <- ifelse(km.out$cluster == 1, "M", "B")
table(cluster.labels, ovarian.data$diagnosis)
mean(cluster.labels == ovarian.data$diagnosis)
```

From the calculated accuracy shown above, we see that there is a good amount of concordance between the identified clusters and the true labels of the cells. The model had an accuracy of 92.16% on the best run.

Additionally, we can calculate the precision and recall of the method by hand for the best run. If we assume that malignant cells are "positive" results, we get the following: true positives (tp) = 205, false negatives (fn) = 35, true negatives (tn) = 371, false positive (fp) = 14.

Then,

Precision = $\frac{tp}{tp+fp}=\frac{205}{205+14}=0.9361$
Recall = $\frac{tp}{tp+fn}=\frac{205}{205+35}=0.8542$

Therefore, the precision of this method is 93.61%, and the recall of this method is 85.42%. Once again, this tells us there there is good concordance between the identified clusters and the true labels of the cells.

\n
**Q2.2. Repeat the k-means analysis 10 times and report the mean accuracy across the 10 runs. Why are the results different in each run?**
\n

```{r, Q2.2}
# Number of repetitions
num_repetitions <- 10

# Vector to store accuracy results
accuracies <- numeric(num_repetitions)

# Repeat k-means 10 times
for(i in 1:10){
  km.out_i <- kmeans(ovarian.scaled, centers = 2, iter.max = 10, nstart = 20)
  cluster.labels_i <- ifelse(km.out_i$cluster == 1, "M", "B")
  print(table(cluster.labels_i, ovarian.data$diagnosis))
  accuracies[i] <- mean(cluster.labels_i == ovarian.data$diagnosis)
}

# Calculate the mean accuracy
mean(accuracies)
```

The results in each run are different because k-means is an iterative algorithm that starts with random initial cluster centers. The initial placement of these centers can influence the final cluster assignments, leading to different results in each run.

\n
**Q2.3. Repeat the same analysis but with the top 5 PCs.**
\n

```{r, Q2.3}
# Take the top 5 PCs
top_5_pcs <- ovarian.pca$x[, 1:5]


# Perform one k-means clustering with 2 clusters
km.out_pcs <- kmeans(top_5_pcs, centers = 2, iter.max = 1, nstart = 20)
cluster.labels_pcs <- ifelse(km.out_pcs$cluster == 1, "M", "B")
table(cluster.labels_pcs, ovarian.data$diagnosis)
mean(cluster.labels_pcs == ovarian.data$diagnosis)

# Now we want to repeat the k-means analysis 10 times
# Vector to store accuracy results
accuracies.pca <- numeric(num_repetitions)

# Perform k-means analysis 10 times
for (i in 1:num_repetitions) {
  km.out_pcs_i <- kmeans(top_5_pcs, centers = 2, iter.max = 10, nstart = 20)
  cluster.labels_pcs_i <- ifelse(km.out_pcs_i$cluster == 1, "M", "B")
  print(table(cluster.labels_pcs_i, ovarian.data$diagnosis))
  accuracies.pca[i] <- mean(cluster.labels_pcs_i == ovarian.data$diagnosis)
}

# Calculate the mean accuracy
mean(accuracies.pca)
```

\n
**Q2.4. Compare the results between Q2.2. and Q2.3.**
\n

The results from Q2.3 were very slightly worse than Q2.2 when comparing a single k-means run. The highest accuracy from Q2.2 was 0.9216, while the highest from Q2.3 was 0.9184. This is because the entire data set is used in Q2.2 and most of the variance in the data is covered.

Alternatively, we can run the code in Q2.2 and Q2.3 multiple times and look at the mean accuracy of repeating k-means 10 times. On some runs, the mean accuracy in Q2.3 is higher than the mean accuracy in Q2.2, suggesting that dimensionality reduction with PCA has captured the most critical information for clustering. On other runs, the mean accuracy in Q2.3 is lower than the mean accuracy in Q2.2, suggesting that the top 5 PCs do not capture enough information for accurate clustering.

Another thing to note is that when we run the code in Q2.2 and Q2.3 multiple times, the mean accuracy in Q2.2 ranges from about 41.57% to 58.43%, while the mean accuracy in Q2.3 ranges from about 33.26% to 75.1%. This suggests that there may be a lot more uncertainty, or variability, in the results when we use the top 5 PCs, indicating that the top 5 PCs may not capture enough information for accurate clustering. We might need more principal components to more accurately cluster the data.

## Q3. Classification

```{r}
# Divide data into training and test sets
ovarian.data.train <- ovarian.data[sample(nrow(ovarian.data))[1:(nrow(ovarian.data)/2)],]
ovarian.data.test <- ovarian.data[sample(nrow(ovarian.data))[(nrow(ovarian.data)/2):(nrow(ovarian.data))],]
```

\n
**Q3.1. Design a logistic regression classifier to identify (differentiate) benign and malignant cells. Report the performance of the classification technique on the training and test sets. You can report accuracy, precision and recall. Compare the performance of the classifier on the training and test set and provide a reason as to why one is better than the other.**
\n

```{r, Q3.1}
# Change diagnosis columns to factors
ovarian.data.train$diagnosis <- as.factor(ovarian.data.train$diagnosis)
ovarian.data.test$diagnosis <- as.factor(ovarian.data.test$diagnosis)

# Create logistic regression model
logistic_model <- glm(diagnosis ~ . -cell_id, data = ovarian.data.train, family = binomial)

# Predict on the training and test sets
train_predictions <- ifelse(predict(logistic_model, ovarian.data.train, type = "response") >= 0.5, "M", "B")
test_predictions <- ifelse(predict(logistic_model, ovarian.data.test, type = "response") >= 0.5, "M", "B")

# Make predictions factors
train_as_factor <- as.factor(train_predictions)
test_as_factor <- as.factor(test_predictions)

# Calculate performance metrics for train data
train_accuracy <- mean(train_as_factor == ovarian.data.train$diagnosis)
train_precision <- posPredValue(train_as_factor, ovarian.data.train$diagnosis, 
                                positive = 'M', negative = 'B')
train_recall <- sensitivity(train_as_factor, ovarian.data.train$diagnosis, positive = "M")

# Print confusion matrix and performance metrics for train data
table(train_as_factor, ovarian.data.train$diagnosis)
print(paste("Train Data Accuracy: ", train_accuracy))
print(paste("Train Data Precision: ", train_precision))
print(paste("Train Data Recall: ", train_recall))

# Calculate performance metrics for test data
test_accuracy <- mean(test_as_factor == ovarian.data.test$diagnosis)
test_precision <- posPredValue(test_as_factor, ovarian.data.test$diagnosis, 
                                positive = 'M', negative = 'B')
test_recall <- sensitivity(test_as_factor, ovarian.data.test$diagnosis, positive = "M")

# Print confusion matrix and performance metrics for test data
table(test_as_factor, ovarian.data.test$diagnosis)
print(paste("Test Data Accuracy: ", test_accuracy))
print(paste("Test Data Precision: ", test_precision))
print(paste("Test Data Recall: ", test_recall))
```

The performance of the classifier on the training set is better than it is on the test set because the model has been trained on that data. The accuracy, precision, and recall of the classifier on the training set is 100%. 

The test set reflects the model's generalization to new data. Since the performance of the classifier on the test set is close to the performance of the classifier on the training set, it suggests that the model generalizes well.

\n
**Q3.2. Repeat the same task as Q3.1. with the top 5 PCs.**
\n

```{r, Q3.2}
# Create the training set for top 5 PCs
ovarian.pca.train.raw <- prcomp(ovarian.data.train[, 3:32], center = TRUE, 
                                scale. = TRUE)
ovarian.pca.train <- as.data.frame(ovarian.pca.train.raw$x[, 1:5])
ovarian.pca.train$Diagnosis <- ovarian.data.train$diagnosis

# Create the test set for top 5 PCs
ovarian.pca.test.raw <- predict(ovarian.pca.train.raw, ovarian.data.test[, 3:32])
ovarian.pca.test <- as.data.frame(ovarian.pca.test.raw[, 1:5])
ovarian.pca.test$Diagnosis <- ovarian.data.test$diagnosis

# Build a logistic regression model
logistic_model_pca <- glm(Diagnosis ~ ., data = ovarian.pca.train, family = binomial)

# Predict on the training and test sets
train_predictions_pca <- ifelse(predict(logistic_model_pca, ovarian.pca.train, type = "response") >= 0.5, "M", "B")
test_predictions_pca <- ifelse(predict(logistic_model_pca, ovarian.pca.test, type = "response") >= 0.5, "M", "B")

# Make predictions factors
train_pca_as_factor <- as.factor(train_predictions_pca)
test_pca_as_factor <- as.factor(test_predictions_pca)

# Calculate performance metrics for train data
train_accuracy_pca <- mean(train_pca_as_factor == ovarian.pca.train$Diagnosis)
train_precision_pca <- posPredValue(train_pca_as_factor, ovarian.pca.train$Diagnosis, 
                                positive = 'M', negative = 'B')
train_recall_pca <- sensitivity(train_pca_as_factor, ovarian.pca.train$Diagnosis, positive = "M")

# Print confusion matrix and performance metrics for train data
table(train_pca_as_factor, ovarian.pca.train$Diagnosis)
print(paste("PCA Train Data Accuracy: ", train_accuracy_pca))
print(paste("PCA Train Data Precision: ", train_precision_pca))
print(paste("PCA Train Data Recall: ", train_recall_pca))

# Calculate performance metrics for test data
test_accuracy_pca <- mean(test_pca_as_factor == ovarian.pca.test$Diagnosis)
test_precision_pca <- posPredValue(test_pca_as_factor, ovarian.pca.test$Diagnosis, 
                                positive = 'M', negative = 'B')
test_recall_pca <- sensitivity(test_pca_as_factor, ovarian.pca.test$Diagnosis, positive = "M")

# Print confusion matrix and performance metrics for test data
table(test_pca_as_factor, ovarian.pca.test$Diagnosis)
print(paste("PCA Test Data Accuracy: ", test_accuracy_pca))
print(paste("PCA Test Data Precision: ", test_precision_pca))
print(paste("PCA Test Data Recall: ", test_recall_pca))
```

In this question, the performance of the classifier on the training set is very slightly better than it is on the test set (except for the recall). This is due to the fact that the model has been trained on that data.

\n
**Q3.3. Compare the results between Q3.1. and Q3.2. Do the results get better or worse? Why?**
\n

Comparing the results of the logistic regression model with and without PCA, we see that the results in Q3.2 are slightly worse than Q3.1 for the training set, but actually slightly better for the test set. With regards to the results for the training set, the results indicate that PCA has not captured enough information for accurate classification, and the original features are essential for the task. Alternatively. with regards to the results for the test set, the results indicate that the model using all the original features has been over-fitted and does not generalize as well to new data in comparison to the model using the top 5 PCs.

\n
**Q3.4. Compare the results of the clustering and classification methods. Which one gives you better result?**
\n

If we compare the accuracy of the clustering and classification methods for non-PCA and PCA data (as accuracy is the only metric that we have computed that is common between all methods), we see that classification is overall higher in accuracy than clustering. This is because classification provides supervised predictions with labels (benign or malignant), while clustering provides unsupervised grouping of data points.

Furthermore, when we ran the clustering method multiple times, we got large variations in accuracy, from a low accuracy in one run to a much higher accuracy in another. But even the highest accuracy we could get was no more than 92.16% when clustering (for the non-PCA clustering model), which is less than both the non-PCA and PCA classification models (96.16% and 97.44% accuracy on test data, respectively). If the aim is to predict cell diagnoses accurately, classification might be the more suitable option.

\n
**Q3.5. Given our ROC curve, what would you say it tells us about the overlap of the two classes? What can we say about the model's separability? How does an ROC curve tell us more about the model's performance than a single sensitivity/specificity measure?**
\n

```{r, Q3.5}
pred.prob <- predict(logistic_model, ovarian.data, type = "response")
predict <- prediction(pred.prob, ovarian.data$diagnosis, label.ordering = c("B","M"))
perform <- performance(predict, "tpr", "fpr")
plot(perform, colorize = TRUE)
```

Given the above ROC curve, we can tell that there is very little overlap between the two classes. The curve is very close to the top-left corner, which indicates that the model does a good job at classifying the data into categories and that the model has very good separability.

The ROC curve provides a more comprehensive view of a model's performance by showing how sensitivity and specificity change with different classification thresholds, which can in turn be used to select an optimal cut-off value for the diagnostic test. It can also help with understanding of the separability of the classes through graphical visualization. In the curve above, we can see that a high true positive rate can be reached with a low false positive rate. This corresponds to a high sensitivity (high true positive rate), while the specificity is also high (low false positive rate).

\n
**Q3.6 . Design another classifier (using a different classification method) and repeat Q3.1-3.**
\n

*(Q3.1) Using Random Forest Classifier*
```{r, Q3.6}
# Make the diagnosis column 1 and 0; 1 for M and 0 for B.
ovarian.data.train.binary <- ovarian.data.train
ovarian.data.train.binary$diagnosis <- ifelse(ovarian.data.train.binary$diagnosis == "M", 1, 0)

# Design a Random Forest classifier on the training set
random_forest_model <- randomForest(diagnosis ~ . -cell_id, data = ovarian.data.train.binary, importance = TRUE)

# Predict on the training and test sets
train_predictions_rf_prob <- predict(random_forest_model, ovarian.data.train, type = "response")
test_predictions_rf_prob <- predict(random_forest_model, ovarian.data.test, type = "response")

# Convert probabilities to class labels
train_predictions_rf <- ifelse(train_predictions_rf_prob >= 0.5, "M", "B")
test_predictions_rf <- ifelse(test_predictions_rf_prob >= 0.5, "M", "B")

# Make predictions factors
train_rf_as_factor <- as.factor(train_predictions_rf)
test_rf_as_factor <- as.factor(test_predictions_rf)

# Calculate performance metrics for train data
train_rf_accuracy <- mean(train_predictions_rf == ovarian.data.train$diagnosis)
train_rf_precision <- posPredValue(train_rf_as_factor, ovarian.data.train$diagnosis, 
                                positive = 'M', negative = 'B')
train_rf_recall <- sensitivity(train_rf_as_factor, ovarian.data.train$diagnosis, positive = "M")

# Print confusion matrix and performance metrics for train data
table(train_rf_as_factor, ovarian.data.train$diagnosis)
print(paste("Train Data Accuracy (RF): ", train_rf_accuracy))
print(paste("Train Data Precision (RF): ", train_rf_precision))
print(paste("Train Data Recall (RF): ", train_rf_recall))

# Calculate performance metrics for test data
test_rf_accuracy <- mean(test_predictions_rf == ovarian.data.test$diagnosis)
test_rf_precision <- posPredValue(test_rf_as_factor, ovarian.data.test$diagnosis, 
                                positive = 'M', negative = 'B')
test_rf_recall <- sensitivity(test_rf_as_factor, ovarian.data.test$diagnosis, positive = "M")

# Print confusion matrix and performance metrics for test data
table(test_rf_as_factor, ovarian.data.test$diagnosis)
print(paste("Test Data Accuracy (RF): ", test_rf_accuracy))
print(paste("Test Data Precision (RF): ", test_rf_precision))
print(paste("Test Data Recall (RF): ", test_rf_recall))
```

Similar to Q3.1, the performance of the RF classifier on the training set is better than it is on the test set because the model has been trained on that data. The accuracy, precision, and recall of the classifier on the training set is 100%. Additionally, since the performance of the classifier on the test set close to the performance of the classifier on the training set, it suggests that the model generalizes well.

*(Q3.2) Using Random Forest Classifier*
```{r}
# Make the diagnosis column 1 and 0; 1 for M and 0 for B.
ovarian.pca.train.binary <- ovarian.pca.train
ovarian.pca.train.binary$Diagnosis <- ifelse(ovarian.pca.train.binary$Diagnosis == "M", 1, 0)

# Design a Random Forest classifier on the training set
random_forest_model_pca <- randomForest(Diagnosis ~ ., ovarian.pca.train.binary, importance = TRUE)

# Predict on the training and test sets
train_predictions_pca_rf_prob <- predict(random_forest_model_pca, ovarian.pca.train, type = "response")
test_predictions_pca_rf_prob <- predict(random_forest_model_pca, ovarian.pca.test, type = "response")

# Convert probabilities to class labels
train_predictions_pca_rf <- ifelse(train_predictions_pca_rf_prob >= 0.5, "M", "B")
test_predictions_pca_rf <- ifelse(test_predictions_pca_rf_prob >= 0.5, "M", "B")

# Make predictions factors
train_pca_rf_as_factor <- as.factor(train_predictions_pca_rf)
test_pca_rf_as_factor <- as.factor(test_predictions_pca_rf)

# Calculate performance metrics for train data
train_pca_rf_accuracy <- mean(train_predictions_pca_rf == ovarian.pca.train$Diagnosis)
train_pca_rf_precision <- posPredValue(train_pca_rf_as_factor, ovarian.pca.train$Diagnosis, 
                                positive = 'M', negative = 'B')
train_pca_rf_recall <- sensitivity(train_pca_rf_as_factor, ovarian.pca.train$Diagnosis, positive = "M")

# Print confusion matrix and performance metrics for train data
table(train_pca_rf_as_factor, ovarian.pca.train$Diagnosis)
print(paste("Train Data Accuracy (RF): ", train_pca_rf_accuracy))
print(paste("Train Data Precision (RF): ", train_pca_rf_precision))
print(paste("Train Data Recall (RF): ", train_pca_rf_recall))

# Calculate performance metrics for test data
test_pca_rf_accuracy <- mean(test_predictions_pca_rf == ovarian.pca.test$Diagnosis)
test_pca_rf_precision <- posPredValue(test_pca_rf_as_factor, ovarian.pca.test$Diagnosis, 
                                positive = 'M', negative = 'B')
test_pca_rf_recall <- sensitivity(test_pca_rf_as_factor, ovarian.pca.test$Diagnosis, positive = "M")

# Print confusion matrix and performance metrics for test data
table(test_pca_rf_as_factor, ovarian.pca.test$Diagnosis)
print(paste("Test Data Accuracy (RF): ", test_pca_rf_accuracy))
print(paste("Test Data Precision (RF): ", test_pca_rf_precision))
print(paste("Test Data Recall (RF): ", test_pca_rf_recall))
```

Similar to the previous part of this question, the performance of the RF classifier on the training set is better than it is on the test set because the model has been trained on that data. The accuracy, precision, and recall of the classifier on the training set is 100%. Additionally, since the performance of the classifier on the test set is close to the performance of the classifier on the training set, it suggests that the model generalizes well.

*(Q3.3) Using Random Forest Classifier*

Comparing the results of Random Forest classification with and without PCA, we see that the results in Q3.2 (with PCA) are very similar to (and even slightly better in terms on accuracy and recall) Q3.1 (without PCA). This suggests that dimensionality reduction using PCA with the Random Forest method is effective in maintaining (and maybe even improving) the classification performance.

## Contributions

All members contributed to coding and reviewing each others' work. Some written questions were worked on together, and the remaining ones divided among group members. The final assignment was reviewed by each group member before submitting.